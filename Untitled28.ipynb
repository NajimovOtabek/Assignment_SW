{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMCdgNgLug7JI66nNZJN3nF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NajimovOtabek/Assignment_SW/blob/main/Untitled28.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkwwiO4elPrt",
        "outputId": "e02b02fa-bc67-4845-88ed-734ff3bb5a7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-t3RdDpmqMs4ABt9oobSapeNYTZJ9tpu\n",
            "To: /content/MachineLearningCSV.zip\n",
            "100% 235M/235M [00:05<00:00, 40.3MB/s]\n",
            "Archive:  MachineLearningCSV.zip\n",
            "   creating: MachineLearningCVE/\n",
            "  inflating: MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Tuesday-WorkingHours.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Friday-WorkingHours-Morning.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv  \n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1-t3RdDpmqMs4ABt9oobSapeNYTZJ9tpu\n",
        "!unzip MachineLearningCSV.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/Jumabek/net_intrusion_detection/develop/preprocessing.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOMBnblllZ3x",
        "outputId": "46a83517-169e-41b0-9cd2-287b58bd24c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-24 08:07:56--  https://raw.githubusercontent.com/Jumabek/net_intrusion_detection/develop/preprocessing.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3790 (3.7K) [text/plain]\n",
            "Saving to: ‘preprocessing.py’\n",
            "\n",
            "preprocessing.py    100%[===================>]   3.70K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-08-24 08:07:57 (76.7 MB/s) - ‘preprocessing.py’ saved [3790/3790]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score, classification_report\n",
        "from preprocessing import read_data, load_data, normalize\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "b1cuZ231la7I"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataroot = 'MachineLearningCVE/'\n",
        "from preprocessing import read_data\n",
        "data = read_data(dataroot,'*.pcap_ISCX.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ez3H8Qk1lcGi",
        "outputId": "41d18914-f172-4393-9238-ff59e13db6fb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[########################################] | 100% Completed | 30.79 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and normalize the data\n",
        "X, y = load_data(dataroot)\n",
        "X = normalize(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kupywtblgjc",
        "outputId": "86af60d7-5c01-42d8-9d32-4324db9bfa4b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[########################################] | 100% Completed | 29.12 s\n",
            "there are 2830743 flow records with 79 feature dimension\n",
            "stripped column names\n",
            "dropped bad columns\n",
            "There are 0 nan entries\n",
            "converted to numeric\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stratified_split(X, y, test_size=0.1, random_state=42, min_class_count=2000):\n",
        "    # Identify classes with at least min_class_count instances\n",
        "    counts = Counter(y)\n",
        "    valid_classes = [cls for cls, count in counts.items() if count >= min_class_count]\n",
        "\n",
        "    # Filter the data to include only those classes\n",
        "    X_filtered = [x for x, cls in zip(X, y) if cls in valid_classes]\n",
        "    y_filtered = [cls for cls in y if cls in valid_classes]\n",
        "\n",
        "    # Stratified sampling to maintain the class distribution\n",
        "    split = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
        "    for train_index, sample_index in split.split(X_filtered, y_filtered):\n",
        "        X_new = [X_filtered[i] for i in sample_index]\n",
        "        y_new = [y_filtered[i] for i in sample_index]\n",
        "\n",
        "    return X_new, y_new\n",
        "\n",
        "def filter_classes(X, y, threshold=100):\n",
        "    # Count the occurrences of each class in y\n",
        "    class_counts = Counter(y)\n",
        "\n",
        "    # Find the classes that meet the threshold\n",
        "    valid_classes = {cls for cls, count in class_counts.items() if count >= threshold}\n",
        "\n",
        "    # Filter the instances of X and y based on the valid classes\n",
        "    X_filtered = [x for x, cls in zip(X, y) if cls in valid_classes]\n",
        "    y_filtered = [cls for cls in y if cls in valid_classes]\n",
        "\n",
        "    return np.array(X_filtered), np.array(y_filtered)\n",
        "\n",
        "# Filter classes with fewer than 100 instances\n",
        "X, y = filter_classes(X, y)"
      ],
      "metadata": {
        "id": "SmC9D9DXlg7y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def balance_data(X, y, target_samples=50000, seed=42):\n",
        "    # Find the minimum class count\n",
        "    min_class_count = min(Counter(y).values())\n",
        "\n",
        "    # Apply under-sampling to reduce all classes to min_class_count\n",
        "    under_sampler = RandomUnderSampler(sampling_strategy='auto', random_state=seed)\n",
        "    X_under, y_under = under_sampler.fit_resample(X, y)\n",
        "\n",
        "    # Apply SMOTE to increase all classes to target_samples\n",
        "    smote_strategy = {label: target_samples for label in np.unique(y_under)}\n",
        "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=seed)\n",
        "    new_X, new_y = smote.fit_resample(X_under, y_under)\n",
        "\n",
        "    return new_X, new_y"
      ],
      "metadata": {
        "id": "-uugzqVLliEB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the Data\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42)\n",
        "\n",
        "# Print class distribution before balancing\n",
        "unique_before, counts_before = np.unique(y, return_counts=True)\n",
        "print(\"Class distribution before balancing:\", dict(zip(unique_before, counts_before)))\n",
        "\n",
        "# Balance the data\n",
        "X_train, y_train = balance_data(X_train, y_train, seed=42)\n",
        "\n",
        "# Print class distribution after balancing\n",
        "unique_after, counts_after = np.unique(y_train, return_counts=True)\n",
        "print(\"Class distribution after balancing:\", dict(zip(unique_after, counts_after)))\n",
        "\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McAjkj9HlkWK",
        "outputId": "088d3903-3b58-4ef7-a6bc-ba743b04e840"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution before balancing: {0: 2273097, 1: 1966, 2: 128027, 3: 10293, 4: 231073, 5: 5499, 6: 5796, 7: 7938, 10: 158930, 11: 5897, 12: 1507, 14: 652}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (50000) in class 0 will be larger than the number of samples in the majority class (class #0 -> 443)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (50000) in class 1 will be larger than the number of samples in the majority class (class #0 -> 443)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (50000) in class 2 will be larger than the number of samples in the majority class (class #0 -> 443)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (50000) in class 3 will be larger than the number of samples in the majority class (class #0 -> 443)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (50000) in class 4 will be larger than the number of samples in the majority class (class #0 -> 443)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (50000) in class 5 will be larger than the number of samples in the majority class (class #0 -> 443)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (50000) in class 6 will be larger than the number of samples in the majority class (class #0 -> 443)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (50000) in class 7 will be larger than the number of samples in the majority class (class #0 -> 443)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (50000) in class 10 will be larger than the number of samples in the majority class (class #0 -> 443)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (50000) in class 11 will be larger than the number of samples in the majority class (class #0 -> 443)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (50000) in class 12 will be larger than the number of samples in the majority class (class #0 -> 443)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (50000) in class 14 will be larger than the number of samples in the majority class (class #0 -> 443)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution after balancing: {0: 50000, 1: 50000, 2: 50000, 3: 50000, 4: 50000, 5: 50000, 6: 50000, 7: 50000, 10: 50000, 11: 50000, 12: 50000, 14: 50000}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Relabel the targets\n",
        "le = LabelEncoder()\n",
        "y_train_relabeled = le.fit_transform(y_train)\n",
        "y_test_relabeled = le.transform(y_test)\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train_relabeled, dtype=torch.long))\n",
        "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test_relabeled, dtype=torch.long))"
      ],
      "metadata": {
        "id": "Qk3XhYgTlnzf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, d_model=64, nhead=4, num_layers=4, dim_feedforward=128, dropout=0.1):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, d_model)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.unsqueeze(1)  # Add a fake batch dimension for the transformer\n",
        "        x = self.transformer(x, x)  # Encoder-Decoder Self-Attention\n",
        "        x = x.squeeze(1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "qnH30VZylocl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
        "\n",
        "def evaluate_model(model, data_loader) -> tuple:\n",
        "    model.eval()\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, labels in data_loader:\n",
        "            data = data.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "            predicted_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    balanced_accuracy = balanced_accuracy_score(true_labels, predicted_labels)\n",
        "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "    precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "    recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "    macro_f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
        "    macro_precision = precision_score(true_labels, predicted_labels, average='macro')\n",
        "    macro_recall = recall_score(true_labels, predicted_labels, average='macro')\n",
        "\n",
        "    print(\"Classification Report : \\n\", classification_report(true_labels, predicted_labels))\n",
        "\n",
        "    return accuracy, balanced_accuracy, f1, precision, recall, macro_f1, macro_precision, macro_recall"
      ],
      "metadata": {
        "id": "GW9uLQhyloaF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(nhead, base_d_model):\n",
        "    # Model Configuration\n",
        "    X_train_array = np.array(X_train)\n",
        "    input_dim = X_train_array.shape[1]\n",
        "    num_classes = len(le.classes_)\n",
        "\n",
        "    # Determine the d_model value based on nhead; ensuring it's divisible by nhead\n",
        "    while base_d_model % nhead != 0:\n",
        "        base_d_model += 1\n",
        "\n",
        "    d_model = base_d_model\n",
        "\n",
        "    model = TransformerModel(input_dim, num_classes, nhead=nhead, d_model=d_model)\n",
        "    model = model.cuda()\n",
        "\n",
        "    # Loss and Optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Dataloader with new batch size\n",
        "    batch_size = 1024\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for i, (data, labels) in enumerate(train_loader):\n",
        "            data = data.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Num heads: {nhead}, d_model: {d_model}, Epoch [{epoch + 1}/10], Loss: {running_loss / len(train_loader)}')\n",
        "\n",
        "    # evaluate model\n",
        "    acc, balanced_acc, f1, prec, rec, macro_f1, macro_prec, macro_rec = evaluate_model(model, DataLoader(test_dataset, batch_size=batch_size))\n",
        "    print(f'Num heads: {nhead}, d_model: {d_model}, Accuracy: {acc:.4f}, Balanced Accuracy: {balanced_acc:.4f}, F1: {f1:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, Macro F1: {macro_f1:.4f}, Macro Precision: {macro_prec:.4f}, Macro Recall: {macro_rec:.4f}')\n",
        "\n",
        "    return acc,balanced_acc, f1, prec, rec, macro_f1, macro_prec, macro_rec\n",
        "\n"
      ],
      "metadata": {
        "id": "c46r_XALloX_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "nheads = [10]\n",
        "d_models = [240]  # Example d_model range, adjust as needed\n",
        "\n",
        "accuracies = []\n",
        "balanced_accuracies = []\n",
        "f1_scores = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "macro_f1_scores = []\n",
        "macro_precisions = []\n",
        "macro_recalls = []\n",
        "\n",
        "results = []\n",
        "\n",
        "for nhead in nheads:\n",
        "    for d_model in d_models:\n",
        "        if d_model % nhead == 0: # Ensure d_model is divisible by nhead\n",
        "            acc, balanced_acc, f1, prec, rec, macro_f1, macro_prec, macro_rec = train_model(nhead, d_model)\n",
        "            results.append((nhead, d_model, acc, balanced_acc, f1, prec, rec, macro_f1, macro_prec, macro_rec))\n",
        "\n",
        "# Convert results into a DataFrame for easy analysis\n",
        "import pandas as pd\n",
        "\n",
        "results_df = pd.DataFrame(results, columns=['nhead', 'd_model', 'Accuracy', 'Balanced Accuracy', 'F1', 'Precision', 'Recall', 'Macro F1', 'Macro Precision', 'Macro Recall'])\n",
        "best_result = results_df.loc[results_df['Accuracy'].idxmax()]\n",
        "\n",
        "print(\"Best Result:\")\n",
        "print(best_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMTwQL5WloVp",
        "outputId": "13ced63b-7f28-4f1e-9acd-a49f0d55876c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num heads: 10, d_model: 240, Epoch [1/10], Loss: 0.9712792166569127\n",
            "Num heads: 10, d_model: 240, Epoch [2/10], Loss: 0.2868493712751939\n",
            "Num heads: 10, d_model: 240, Epoch [3/10], Loss: 0.2819905995356345\n",
            "Num heads: 10, d_model: 240, Epoch [4/10], Loss: 0.22924224783129252\n",
            "Num heads: 10, d_model: 240, Epoch [5/10], Loss: 0.3477036595700544\n",
            "Num heads: 10, d_model: 240, Epoch [6/10], Loss: 0.2776769884566398\n",
            "Num heads: 10, d_model: 240, Epoch [7/10], Loss: 0.2629396391606575\n",
            "Num heads: 10, d_model: 240, Epoch [8/10], Loss: 0.23247535247037843\n",
            "Num heads: 10, d_model: 240, Epoch [9/10], Loss: 0.23556215789029217\n",
            "Num heads: 10, d_model: 240, Epoch [10/10], Loss: 0.2231332288972347\n",
            "Classification Report : \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.79      0.88    340873\n",
            "           1       0.01      0.97      0.03       277\n",
            "           2       0.94      0.92      0.93     19142\n",
            "           3       0.09      0.98      0.16      1573\n",
            "           4       0.82      0.88      0.85     34724\n",
            "           5       0.36      0.93      0.52       821\n",
            "           6       0.09      0.91      0.16       875\n",
            "           7       0.26      0.94      0.41      1205\n",
            "           8       0.73      0.96      0.83     23945\n",
            "           9       0.13      0.87      0.23       852\n",
            "          10       0.01      0.08      0.01       215\n",
            "          11       0.03      0.81      0.05       100\n",
            "\n",
            "    accuracy                           0.81    424602\n",
            "   macro avg       0.37      0.84      0.42    424602\n",
            "weighted avg       0.95      0.81      0.87    424602\n",
            "\n",
            "Num heads: 10, d_model: 240, Accuracy: 0.8138, Balanced Accuracy: 0.8359, F1: 0.8664, Precision: 0.9466, Recall: 0.8138, Macro F1: 0.4211, Macro Precision: 0.3713, Macro Recall: 0.8359\n",
            "Best Result:\n",
            "nhead                 10.000000\n",
            "d_model              240.000000\n",
            "Accuracy               0.813797\n",
            "Balanced Accuracy      0.835924\n",
            "F1                     0.866432\n",
            "Precision              0.946634\n",
            "Recall                 0.813797\n",
            "Macro F1               0.421142\n",
            "Macro Precision        0.371335\n",
            "Macro Recall           0.835924\n",
            "Name: 0, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Rxt6Q4YloTS"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}